{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.dates as mdates\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree,export_text, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.sandwich_covariance as sw\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "import math\n",
    "\n",
    "import scipy.stats as st\n",
    "# import mord\n",
    "from scipy import stats \n",
    "from scipy.stats import t\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# from tabulate import tabulate\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression \n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats.sandwich_covariance as sw\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv('forXgboost_addiction.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concated  = pd.read_csv('./data/df_concated.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_dict = {'M': 0, 'F': 1,0:0,1:1}\n",
    "# gender_dict = {'M': 1, 'F': 2}\n",
    "gender_str_dict = {'男': 'M', '女':'F' }\n",
    "sex_dict = {2: 1, 1: 0}\n",
    "\n",
    "\n",
    "age_segment_dict = {\"0-11\": 1, \"12-17\": 2, \"18-23\": 3, \"24-30\": 4, \"31-40\": 5, \"41-49\": 6, \"50+\": 7}\n",
    "fre_city_level_dict = {\n",
    "    '新一线城市': 1,\n",
    "    '一线城市': 2,\n",
    "    '二线城市': 3,\n",
    "    '三线城市': 4,\n",
    "    '四线城市': 5,\n",
    "    '五线城市': 6\n",
    "}\n",
    "\n",
    "df_all['gender'] = df_all['gender'].map(gender_dict).astype(float)\n",
    "df_all['sex_str'] = df_all['sex_str'].map(gender_str_dict)\n",
    "df_all['sex'] = df_all['sex'].map(sex_dict)\n",
    "\n",
    "df_all['age_segment'] = df_all['age_segment'].map(age_segment_dict).astype(float)\n",
    "\n",
    "\n",
    "\n",
    "df_all['fre_city_level'] = df_all['fre_city_level'].map(fre_city_level_dict).astype(float)\n",
    "\n",
    "\n",
    "# Define the age_segment bins and corresponding labels\n",
    "bins = [0, 11, 17, 23, 30, 40, 49, np.inf]\n",
    "labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Fill missing 'age_segment' using pd.cut(), but only for rows where 'age_segment' is NaN\n",
    "df_all['age_segment'] = np.where(\n",
    "    df_all['age_segment'].isna(), \n",
    "    pd.cut(df_all['age'], bins=bins, labels=labels), \n",
    "    df_all['age_segment']\n",
    ")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify the changes\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_all.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[['addiction_1', 'addiction_2', 'addiction_3', 'addiction_4', 'addiction_5', 'addiction_6',\n",
    "        'addiction_score', 'soft_criteria', 'hard_criteria', 'age', 'gender', \n",
    "        'age_survey', 'sex', 'fre_city_level', \n",
    "        'mod_price', 'database', 'submit_time']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['user_id'] = df_all['user_id'].astype(str)\n",
    "print(df_all['user_id'].nunique())\n",
    "df_all = df_all.drop_duplicates(subset='user_id', keep='first')\n",
    "print(df_all['user_id'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby('soft_criteria')['user_id'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby('hard_criteria')['user_id'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "p_values = {}\n",
    "\n",
    "y = 'hard_criteria'\n",
    "for item in [\n",
    "    \"age\",\n",
    "    \"age_survey\",\n",
    "\n",
    "    'C(age_segment)',\n",
    "    'C(gender)',\n",
    "    'C(fre_city_level)',\n",
    "    # 'mod_price',\n",
    "    \n",
    "    'hour_0_wt',\n",
    " 'hour_1_wt',\n",
    " 'hour_2_wt',\n",
    " 'hour_3_wt',\n",
    " 'hour_4_wt',\n",
    " 'hour_5_wt',\n",
    " 'hour_6_wt',\n",
    " 'hour_7_wt',\n",
    " 'hour_8_wt',\n",
    " 'hour_9_wt',\n",
    " 'hour_10_wt',\n",
    " 'hour_11_wt',\n",
    " 'hour_12_wt',\n",
    " 'hour_13_wt',\n",
    " 'hour_14_wt',\n",
    " 'hour_15_wt',\n",
    " 'hour_16_wt',\n",
    " 'hour_17_wt',\n",
    " 'hour_18_wt',\n",
    " 'hour_19_wt',\n",
    " 'hour_20_wt',\n",
    " 'hour_21_wt',\n",
    " 'hour_22_wt',\n",
    " 'hour_23_wt',\n",
    " 'morning_wt',\n",
    " 'noon_wt',\n",
    " 'afternoon_wt',\n",
    " 'evening_wt',\n",
    " 'midnight_wt',\n",
    "    'watch_all_per_day', \n",
    "    \"watch_all_per_week\",\n",
    "    \"watch_all_per_2weeks\", \n",
    "    'watch_all_per_session',\n",
    "    'coverage_per_week',\n",
    "    'coverage_per_2weeks',\n",
    "    \"category_count_unique\",\n",
    "    \"root_category_count_unique\",\n",
    "\n",
    "    'session_all_per_day',\n",
    "    'pid_SHUA_all_per_day',\n",
    "    'pid_watch_all_per_day',\n",
    "    # 'session_all_per_2weeks',\n",
    "    # 'pid_SHUA_all_per_2weeks',\n",
    "    # 'pid_watch_all_per_2weeks',\n",
    "    'duration_per_day',\n",
    "    'longvideo_count_per_day',\n",
    "    'shortvideo_count_per_day',\n",
    "\n",
    "    #  'pastSpent_self',\n",
    "    # 'futureSpent_self',\n",
    "]:\n",
    "    \n",
    "    logit_mod = smf.logit(formula=f'{y} ~ {item}', data=df_all)\n",
    "    logit_res = logit_mod.fit()\n",
    "    \n",
    "    # Print the results summary\n",
    "    print(\"Model =\", f'{y} ~ {item}', '\\n', logit_res.summary())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    p_values[item] = logit_res.pvalues[1]  # Extract p-value for the independent variable\n",
    "\n",
    "\n",
    "    # Sort p-values and take the top 10 lowest\n",
    "    sorted_p_values = dict(sorted(p_values.items(), key=lambda x: x[1])[:])\n",
    "\n",
    "# Plot the top 10 lowest p-values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(sorted_p_values.keys(), sorted_p_values.values())\n",
    "plt.axhline(y=0.05, color='r', linestyle='--')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.ylabel(\"p-value\", fontsize=14)\n",
    "plt.title(\"Top 10 Lowest p-values from Logistic Regression\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(sorted_p_values.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[[\n",
    "    \"age\",\n",
    "    'gender',\n",
    "    'watch_all_per_day', \n",
    "    'coverage_per_2weeks',\n",
    "    \"category_count_unique\",\n",
    "    'session_all_per_day',\n",
    "    'midnight_wt',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model for addicted vs non-addicted\n",
    "SoftOrHard_criteria = 'soft_criteria'\n",
    "print(f'**** {SoftOrHard_criteria} ****')\n",
    "\n",
    "# Define target and feature columns\n",
    "target_column = SoftOrHard_criteria\n",
    "feature_column = [\n",
    "    \"age\", 'gender', 'watch_all_per_day', 'coverage_per_2weeks',\n",
    "    \"category_count_unique\", \n",
    "    'session_all_per_day', 'midnight_wt',\n",
    "    'morning_wt', \n",
    "    'noon_wt', \n",
    "    'afternoon_wt', \n",
    "    'evening_wt',\n",
    "    # scrolling+ watch unique videos vs watch >=1 second(s) unique videos\n",
    "    'pid_SHUA_all_per_day', \n",
    "    'pid_watch_all_per_day',\n",
    "]\n",
    "\n",
    "# Define XGBoost parameters\n",
    "param = {\n",
    "    'max_depth': 6, 'min_child_weight': 1, 'gamma': 0, 'eta': 0.1,\n",
    "    'colsample_bytree': 0.7, 'subsample': 0.7, 'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic', 'alpha': 0.3, 'lambda': 2.0, 'seed': 42\n",
    "}\n",
    "num_round = 110\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df_all[SoftOrHard_criteria].value_counts())\n",
    "\n",
    "# Initialize dictionaries to store metrics across folds\n",
    "metrics_all_train_folds = {\n",
    "    'train_accuracy': [], 'train_precision': [], 'train_recall': [], 'train_f1_score': [], 'train_roc_auc': []\n",
    "}\n",
    "\n",
    "metrics_all_folds = {\n",
    "    'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []\n",
    "}\n",
    "feature_importances = {feature: 0 for feature in feature_column}\n",
    "\n",
    "# Set up k-Fold Cross-Validation\n",
    "smote = SMOTE(random_state=42)\n",
    "k = 5  # Number of folds\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_index = 1\n",
    "\n",
    "# Perform k-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(df_all, df_all[SoftOrHard_criteria]):\n",
    "    print(f\"\\n--- Fold {fold_index}/{k} ---\")\n",
    "    df_train = df_all.iloc[train_index]\n",
    "    df_test = df_all.iloc[test_index]\n",
    "\n",
    "    # Prepare data for XGBoost\n",
    "    xg_train = xgb.DMatrix(df_train[feature_column], label=df_train[SoftOrHard_criteria])\n",
    "    xg_test = xgb.DMatrix(df_test[feature_column], label=df_test[SoftOrHard_criteria])\n",
    "\n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    pos_count = sum(df_train[SoftOrHard_criteria] == 1)\n",
    "    neg_count = sum(df_train[SoftOrHard_criteria] == 0)\n",
    "    param['scale_pos_weight'] = neg_count / pos_count\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    bst = xgb.train(param, xg_train, num_round)\n",
    "\n",
    "     # --- Training Set Evaluation ---\n",
    "    preds_train = bst.predict(xg_train)\n",
    "    y_pred_train = (preds_train >= 0.5).astype(int)\n",
    "    y_train = df_train[SoftOrHard_criteria]\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    train_prec = precision_score(y_train, y_pred_train)\n",
    "    train_recall = recall_score(y_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_train, preds_train)\n",
    "\n",
    "    # Store train metrics\n",
    "    metrics_all_train_folds['train_accuracy'].append(train_acc)\n",
    "    metrics_all_train_folds['train_precision'].append(train_prec)\n",
    "    metrics_all_train_folds['train_recall'].append(train_recall)\n",
    "    metrics_all_train_folds['train_f1_score'].append(train_f1)\n",
    "    metrics_all_train_folds['train_roc_auc'].append(train_roc_auc)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Train Precision: {train_prec:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "    print(f\"Train ROC AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Test Set Evaluation ---\n",
    "    preds_test = bst.predict(xg_test)\n",
    "    y_pred_test = (preds_test >= 0.5).astype(int)\n",
    "    y_test = df_test[SoftOrHard_criteria]\n",
    "\n",
    "\n",
    "    # Evaluate metrics for this fold\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_prec = precision_score(y_test, y_pred_test)\n",
    "    test_recall = recall_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_test, preds_test)\n",
    "\n",
    "    metrics_all_folds['accuracy'].append(test_acc)\n",
    "    metrics_all_folds['precision'].append(test_prec)\n",
    "    metrics_all_folds['recall'].append(test_recall)\n",
    "    metrics_all_folds['f1_score'].append(test_f1)\n",
    "    metrics_all_folds['roc_auc'].append(test_roc_auc)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_prec:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    # Save as Pickle\n",
    "    pkl_path = os.path.join(f'./addict1_xgboost/{SoftOrHard_criteria}_xgboost_model_{fold_index}.pkl')\n",
    "    with open(pkl_path, 'wb') as model_file:\n",
    "        pickle.dump(bst, model_file)\n",
    "\n",
    "    # Accumulate feature importance\n",
    "    importance_scores = bst.get_score(importance_type='weight')\n",
    "    for feature, importance in importance_scores.items():\n",
    "        feature_importances[feature] += importance\n",
    "\n",
    "    fold_index += 1\n",
    "\n",
    "# Average feature importances across folds\n",
    "avg_importances = {feature: importance / k for feature, importance in feature_importances.items()}\n",
    "sorted_avg_importances = sorted(avg_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Compute summary metrics\n",
    "metrics_summary = {}\n",
    "metrics_std = {}\n",
    "for metric_name, metric_values in metrics_all_folds.items():\n",
    "    metrics_summary[metric_name] = {\n",
    "        'best': max(metric_values),\n",
    "        'average': np.mean(metric_values),\n",
    "        'lowest': min(metric_values),\n",
    "    }\n",
    "    metrics_std[metric_name] = np.std(metric_values)\n",
    "\n",
    "# Display metrics summary\n",
    "print(\"\\n=== Metrics Summary over k-Fold Cross-Validation ===\")\n",
    "for metric_name, summary in metrics_summary.items():\n",
    "    print(f\"{metric_name.capitalize()}: Best = {summary['best']:.4f}, Average = {summary['average']:.4f}, Std = {metrics_std[metric_name]:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "metrics = list(metrics_summary.keys())\n",
    "best_values = [metrics_summary[m]['best'] for m in metrics]\n",
    "average_values = [metrics_summary[m]['average'] for m in metrics]\n",
    "lowest_values = [metrics_summary[m]['lowest'] for m in metrics]\n",
    "std_values = [metrics_std[m] for m in metrics]\n",
    "\n",
    "\n",
    "\n",
    "x = np.arange(len(metrics))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "best_color = '#AC98E1'\n",
    "average_color = '#B7D1F1'\n",
    "lowest_color = '#F9C5B3'\n",
    "\n",
    "rects1 = ax.bar(x - width, best_values, width, label='Best', color=best_color)\n",
    "rects2 = ax.bar(x, average_values, width, label='Average', color=average_color)\n",
    "rects3 = ax.bar(x + width, lowest_values, width, label='Lowest', color=lowest_color)\n",
    "\n",
    "# # Use colors from the 'tab10' colormap\n",
    "# colors = sns.color_palette(\"Set2\") \n",
    "\n",
    "\n",
    "# Adding labels, title, and custom x-axis tick labels\n",
    "ax.set_ylabel('Metric Value', fontsize=18)\n",
    "ax.set_title('XGBoost Performance with k-Fold Cross-Validation', fontsize=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([metric.capitalize() for metric in metrics], fontsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "# Adjusting the legend placement to the upper center outside the plot\n",
    "ax.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, -0.12), fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "# Function to attach labels on the bars with spacing\n",
    "def autolabel(rects, spacing=5):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, spacing),  # Use spacing for vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=14, color='black')\n",
    "\n",
    "# Attach labels to each bar with spacing\n",
    "autolabel(rects1, spacing=10)\n",
    "autolabel(rects2, spacing=10)\n",
    "autolabel(rects3, spacing=10)\n",
    "\n",
    "# Set y-axis limits for clarity\n",
    "plt.ylim(0, 1.05)  # Set slightly above 1 for clearer labels\n",
    "\n",
    "# Add grid lines only on the y-axis\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout for better fit and aesthetics\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot average metrics with standard deviation as error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the average values with error bars showing the standard deviation\n",
    "bars = ax.bar(metrics, average_values, yerr=std_values, capsize=8, color='lightblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add labels to the bars\n",
    "for bar, avg, std in zip(bars, average_values, std_values):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{avg:.4f} ± {std:.4f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height*1.13),\n",
    "                xytext=(0, 5),  # Offset for label position\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=14, color='black')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('Addicted vs Non-Addicted: XGBoost Performance with Standard Deviation', fontsize=16)\n",
    "ax.set_ylabel('Metric Value', fontsize=14)\n",
    "ax.set_xlabel('Performance Metrics', fontsize=14)\n",
    "ax.set_ylim(0, 1.05)  # Ensure bars fit in the plot\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Add grid lines for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### decision tree #####\n",
    "\n",
    "df_train_best, df_test_best = train_test_split(\n",
    "    df_all, \n",
    "    test_size=0.2, \n",
    "    random_state=42,  # Best tryout index or fixed seed\n",
    "    stratify=df_all[SoftOrHard_criteria]\n",
    ")\n",
    "\n",
    "# Prepare X and y for the Decision Tree\n",
    "X_best = df_train_best[feature_column]  # Features for training\n",
    "y_best = df_train_best[SoftOrHard_criteria]  # Target variable for training\n",
    "X_test_best = df_test_best[feature_column]  # Features for testing\n",
    "y_test_best = df_test_best[SoftOrHard_criteria]  # Target variable for testing\n",
    "\n",
    "best_roc_auc = 0\n",
    "best_accuracy = 0\n",
    "best_precision = 0\n",
    "best_recall = 0\n",
    "best_f1 = 0\n",
    "best_max_depth = None\n",
    "best_min_samples_leaf = None\n",
    "best_dt_model = None\n",
    "\n",
    "# Perform Decision Tree tryouts\n",
    "for i in [3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "    for j in [2, 3, 4, 5, 6]:\n",
    "        print('max_depth: ', i, '; min_samples_leaf: ', j)\n",
    "        \n",
    "        # Initialize and fit the Decision Tree model\n",
    "        dt_model = DecisionTreeClassifier(max_depth=i, min_samples_leaf=j, random_state=42, class_weight='balanced')\n",
    "        dt_model.fit(X_best, y_best)\n",
    "\n",
    "        # Predict on the test set\n",
    "        X_test_best = df_test_best[feature_column]\n",
    "        y_test_best = df_test_best[SoftOrHard_criteria]\n",
    "        y_pred_dt = dt_model.predict(X_test_best)\n",
    "\n",
    "        # Evaluate the model\n",
    "        dt_roc_auc = roc_auc_score(y_test_best, y_pred_dt)\n",
    "        \n",
    "        if dt_roc_auc > best_roc_auc:\n",
    "            best_roc_auc = dt_roc_auc\n",
    "            best_dt_model = dt_model\n",
    "            best_max_depth = i\n",
    "            best_min_samples_leaf = j\n",
    "\n",
    "# Print best model details\n",
    "print(\"\\n*** Decision Tree Model with Best ROC AUC: ***\")\n",
    "print(f\"max_depth: {best_max_depth}\")\n",
    "print(f\"min_samples_leaf: {best_min_samples_leaf}\")\n",
    "print(f\"Best ROC AUC: {best_roc_auc:.4f}\")\n",
    "\n",
    "# --- **Feature Importance Calculation** ---\n",
    "# Calculate feature importance for the best Decision Tree model\n",
    "feature_importances = best_dt_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_column,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\n=== Feature Importances ===\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# --- **Feature Importance Visualization** ---\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.barh(feature_importance_df['Feature'][:10][::-1], feature_importance_df['Importance'][:10][::-1], color='skyblue')\n",
    "plt.title(\"Top 10 Feature Importances (Decision Tree)\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addiction_columns = ['addiction_1', 'addiction_2', 'addiction_3', 'addiction_4', 'addiction_5', 'addiction_6']\n",
    "# Define addiction columns\n",
    "\n",
    "# Condition: At least 5 questions with a score of 3 or more\n",
    "df_all['at_least_5_questions_3plus'] = (df_all[addiction_columns] >= 3).sum(axis=1) >= 5\n",
    "\n",
    "# Combine the condition for addiction_score >= 18 and at_least_5_questions_3plus\n",
    "condition = (df_all['soft_criteria'] == 1) & (df_all['addiction_score'] >= 18) & df_all['at_least_5_questions_3plus']\n",
    "\n",
    "# Create a new column 'hard_criteria_18_atleast5Q' with 1 for rows fulfilling the condition, otherwise 0\n",
    "df_all['hard_criteria_18_atleast5Q'] = condition.astype(int)\n",
    "\n",
    "# Debug: Count unique users in each category of 'hard_criteria_18_atleast5Q'\n",
    "print(\"\\nUnique users by 'hard_criteria_18_atleast5Q':\")\n",
    "print(df_all.groupby('hard_criteria_18_atleast5Q')['user_id'].nunique())\n",
    "\n",
    "# Assign 3-label criteria\n",
    "conditions = [\n",
    "    (df_all['hard_criteria_18_atleast5Q'] == 1),  # Label 2: Hard addiction\n",
    "    (df_all['soft_criteria'] == 1) & (df_all['hard_criteria_18_atleast5Q'] == 0),  # Label 1: Soft addiction but not hard addiction\n",
    "    (df_all['soft_criteria'] == 0)  # Label 0: Non-addicted\n",
    "]\n",
    "\n",
    "choices = [2, 1, 0]\n",
    "\n",
    "# Create the new column '3_label_criteria_18_atleast5Q'\n",
    "df_all['3_label_criteria_18_atleast5Q'] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "# Debug: Check value counts for the new column\n",
    "print(\"\\nValue counts for '3_label_criteria_18_atleast5Q':\")\n",
    "print(df_all['3_label_criteria_18_atleast5Q'].value_counts())\n",
    "\n",
    "\n",
    "df_all_hard_vs_soft = df_all[df_all['soft_criteria'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## severe vs mildly addicted people \n",
    "SoftOrHard_criteria = 'hard_criteria'\n",
    "print(f'**** {SoftOrHard_criteria} ****')\n",
    "# Define target and feature columns\n",
    "target_column = SoftOrHard_criteria\n",
    "feature_column = [\n",
    "    \"age\", 'gender', \n",
    "    'watch_all_per_day',\n",
    "    'coverage_per_2weeks',\n",
    "    \"category_count_unique\",\n",
    "    'session_all_per_day', \n",
    "    'midnight_wt',\n",
    "    'morning_wt', 'noon_wt', 'afternoon_wt', \n",
    "    'evening_wt',\n",
    "    # # scrolling+ watch unique videos vs watch >=1 second(s) unique videos\n",
    "    'pid_SHUA_all_per_day', 'pid_watch_all_per_day',\n",
    "]\n",
    "\n",
    "# Define XGBoost parameters\n",
    "param = {\n",
    "    'max_depth': 3, 'min_child_weight': 5, 'gamma': 2, 'eta': 0.01,\n",
    "    'colsample_bytree': 0.7, 'subsample': 0.6, 'eval_metric': 'auc',\n",
    "    'objective': 'binary:logistic', 'alpha': 0.3, 'lambda': 2.0, 'seed': 42\n",
    "}\n",
    "num_round = 130\n",
    "\n",
    "\n",
    "\n",
    "# Display class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(df_all_hard_vs_soft[SoftOrHard_criteria].value_counts())\n",
    "\n",
    "# Initialize dictionaries to store metrics across folds\n",
    "metrics_all_train_folds = {\n",
    "    'train_accuracy': [], 'train_precision': [], 'train_recall': [], 'train_f1_score': [], 'train_roc_auc': []\n",
    "}\n",
    "\n",
    "metrics_all_folds = {\n",
    "    'accuracy': [], 'precision': [], 'recall': [], 'f1_score': [], 'roc_auc': []\n",
    "}\n",
    "feature_importances = {feature: 0 for feature in feature_column}\n",
    "\n",
    "# Set up k-Fold Cross-Validation\n",
    "smote = SMOTE(random_state=42)\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "k = 5  # Number of folds\n",
    "skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "fold_index = 1\n",
    "\n",
    "# Perform k-Fold Cross-Validation\n",
    "for train_index, test_index in skf.split(df_all_hard_vs_soft, df_all_hard_vs_soft[SoftOrHard_criteria]):\n",
    "    print(f\"\\n--- Fold {fold_index}/{k} ---\")\n",
    "    df_train = df_all_hard_vs_soft.iloc[train_index]\n",
    "    df_test = df_all_hard_vs_soft.iloc[test_index]\n",
    "\n",
    "    print(f\"Train set size: {len(df_train)}\")\n",
    "    print(f\"Test set size: {len(df_test)}\")\n",
    "\n",
    "    # Prepare data for XGBoost\n",
    "    xg_train = xgb.DMatrix(df_train[feature_column], label=df_train[SoftOrHard_criteria])\n",
    "    xg_test = xgb.DMatrix(df_test[feature_column], label=df_test[SoftOrHard_criteria])\n",
    "\n",
    "    # Calculate scale_pos_weight for imbalance\n",
    "    pos_count = sum(df_train[SoftOrHard_criteria] == 1)\n",
    "    neg_count = sum(df_train[SoftOrHard_criteria] == 0)\n",
    "    param['scale_pos_weight'] = neg_count / pos_count\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    bst = xgb.train(param, xg_train, num_round)\n",
    "\n",
    "    # --- Training Set Evaluation ---\n",
    "    preds_train = bst.predict(xg_train)\n",
    "    y_pred_train = (preds_train >= 0.5).astype(int)\n",
    "    y_train = df_train[SoftOrHard_criteria]\n",
    "\n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    train_prec = precision_score(y_train, y_pred_train)\n",
    "    train_recall = recall_score(y_train, y_pred_train)\n",
    "    train_f1 = f1_score(y_train, y_pred_train)\n",
    "    train_roc_auc = roc_auc_score(y_train, preds_train)\n",
    "\n",
    "    # Store train metrics\n",
    "    metrics_all_train_folds['train_accuracy'].append(train_acc)\n",
    "    metrics_all_train_folds['train_precision'].append(train_prec)\n",
    "    metrics_all_train_folds['train_recall'].append(train_recall)\n",
    "    metrics_all_train_folds['train_f1_score'].append(train_f1)\n",
    "    metrics_all_train_folds['train_roc_auc'].append(train_roc_auc)\n",
    "\n",
    "    print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Train Precision: {train_prec:.4f}\")\n",
    "    print(f\"Train Recall: {train_recall:.4f}\")\n",
    "    print(f\"Train F1 Score: {train_f1:.4f}\")\n",
    "    print(f\"Train ROC AUC: {train_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Test Set Evaluation ---\n",
    "    preds_test = bst.predict(xg_test)\n",
    "    y_pred_test = (preds_test >= 0.5).astype(int)\n",
    "    y_test = df_test[SoftOrHard_criteria]\n",
    "\n",
    "\n",
    "    # Evaluate metrics for this fold\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_prec = precision_score(y_test, y_pred_test)\n",
    "    test_recall = recall_score(y_test, y_pred_test)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    test_roc_auc = roc_auc_score(y_test, preds_test)\n",
    "\n",
    "    metrics_all_folds['accuracy'].append(test_acc)\n",
    "    metrics_all_folds['precision'].append(test_prec)\n",
    "    metrics_all_folds['recall'].append(test_recall)\n",
    "    metrics_all_folds['f1_score'].append(test_f1)\n",
    "    metrics_all_folds['roc_auc'].append(test_roc_auc)\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"Test Precision: {test_prec:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n",
    "\n",
    "    pkl_path = os.path.join(f'./addict1_xgboost/{SoftOrHard_criteria}_xgboost_model_{fold_index}.pkl')\n",
    "    with open(pkl_path, 'wb') as model_file:\n",
    "        pickle.dump(bst, model_file)\n",
    "\n",
    "    # Accumulate feature importance\n",
    "    importance_scores = bst.get_score(importance_type='weight')\n",
    "    for feature, importance in importance_scores.items():\n",
    "        feature_importances[feature] += importance\n",
    "\n",
    "    fold_index += 1\n",
    "\n",
    "# Average feature importances across folds\n",
    "avg_importances = {feature: importance / k for feature, importance in feature_importances.items()}\n",
    "sorted_avg_importances = sorted(avg_importances.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Compute summary metrics\n",
    "metrics_summary = {}\n",
    "metrics_std = {}\n",
    "for metric_name, metric_values in metrics_all_folds.items():\n",
    "    metrics_summary[metric_name] = {\n",
    "        'best': max(metric_values),\n",
    "        'average': np.mean(metric_values),\n",
    "        'lowest': min(metric_values),\n",
    "    }\n",
    "    metrics_std[metric_name] = np.std(metric_values)\n",
    "\n",
    "# Display metrics summary\n",
    "print(\"\\n=== Metrics Summary over k-Fold Cross-Validation ===\")\n",
    "for metric_name, summary in metrics_summary.items():\n",
    "    print(f\"{metric_name.capitalize()}: Best = {summary['best']:.4f}, Average = {summary['average']:.4f}, Std = {metrics_std[metric_name]:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "metrics = list(metrics_summary.keys())\n",
    "best_values = [metrics_summary[m]['best'] for m in metrics]\n",
    "average_values = [metrics_summary[m]['average'] for m in metrics]\n",
    "lowest_values = [metrics_summary[m]['lowest'] for m in metrics]\n",
    "std_values = [metrics_std[m] for m in metrics]\n",
    "\n",
    "\n",
    "\n",
    "x = np.arange(len(metrics))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "best_color = '#AC98E1'\n",
    "average_color = '#B7D1F1'\n",
    "lowest_color = '#F9C5B3'\n",
    "\n",
    "rects1 = ax.bar(x - width, best_values, width, label='Best', color=best_color)\n",
    "rects2 = ax.bar(x, average_values, width, label='Average', color=average_color)\n",
    "rects3 = ax.bar(x + width, lowest_values, width, label='Lowest', color=lowest_color)\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', label='Random Guess')\n",
    "\n",
    "\n",
    "\n",
    "# Adding labels, title, and custom x-axis tick labels\n",
    "ax.set_ylabel('Metric Value', fontsize=18)\n",
    "ax.set_title('Severely vs Mildly Addicted Users: XGBoost Performance with k-Fold Cross-Validation', fontsize=20)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([metric.capitalize() for metric in metrics], fontsize=18)\n",
    "ax.tick_params(axis='y', labelsize=18)\n",
    "\n",
    "# Adjusting the legend placement to the upper center outside the plot\n",
    "ax.legend(fontsize=16, loc='upper center', bbox_to_anchor=(0.5, -0.12), fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "# Function to attach labels on the bars with spacing\n",
    "def autolabel(rects, spacing=5):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, spacing),  # Use spacing for vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=14, color='black')\n",
    "\n",
    "# Attach labels to each bar with spacing\n",
    "autolabel(rects1, spacing=10)\n",
    "autolabel(rects2, spacing=10)\n",
    "autolabel(rects3, spacing=10)\n",
    "\n",
    "# Set y-axis limits for clarity\n",
    "plt.ylim(0, 1.05)  # Set slightly above 1 for clearer labels\n",
    "\n",
    "# Add grid lines only on the y-axis\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout for better fit and aesthetics\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot average metrics with standard deviation as error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot the average values with error bars showing the standard deviation\n",
    "bars = ax.bar(metrics, average_values, yerr=std_values, capsize=8, color='lightblue', alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add labels to the bars\n",
    "for bar, avg, std in zip(bars, average_values, std_values):\n",
    "    height = bar.get_height()\n",
    "    ax.annotate(f'{avg:.4f} ± {std:.4f}',\n",
    "                xy=(bar.get_x() + bar.get_width() / 2, height+0.19),\n",
    "                xytext=(0, 5),  # Offset for label position\n",
    "                textcoords=\"offset points\",\n",
    "                ha='center', va='bottom', fontsize=14, color='black')\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_title('XGBoost Performance with Standard Deviation', fontsize=16)\n",
    "ax.set_ylabel('Metric Value', fontsize=14)\n",
    "ax.set_xlabel('Performance Metrics', fontsize=14)\n",
    "ax.set_ylim(0, 1.05)  # Ensure bars fit in the plot\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Add grid lines for clarity\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10000 = pd.read_csv('10000_user_data.csv',encoding='utf-8')\n",
    "df_10000 = df_10000.rename(columns={'age_kuaishou': 'age'})\n",
    "df_10000['user_id'] = df_10000['user_id'].astype(str)\n",
    "\n",
    "df_survey = pd.read_csv('df_survey.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = df_all.columns\n",
    "\n",
    "common_columns=[ 'user_id','hour_0_wt',\n",
    " 'hour_1_wt',\n",
    " 'hour_2_wt',\n",
    " 'hour_3_wt',\n",
    " 'hour_4_wt',\n",
    " 'hour_5_wt',\n",
    " 'hour_6_wt',\n",
    " 'hour_7_wt',\n",
    " 'hour_8_wt',\n",
    " 'hour_9_wt',\n",
    " 'hour_10_wt',\n",
    " 'hour_11_wt',\n",
    " 'hour_12_wt',\n",
    " 'hour_13_wt',\n",
    " 'hour_14_wt',\n",
    " 'hour_15_wt',\n",
    " 'hour_16_wt',\n",
    " 'hour_17_wt',\n",
    " 'hour_18_wt',\n",
    " 'hour_19_wt',\n",
    " 'hour_20_wt',\n",
    " 'hour_21_wt',\n",
    " 'hour_22_wt',\n",
    " 'hour_23_wt',\n",
    " 'morning_wt',\n",
    " 'noon_wt',\n",
    " 'afternoon_wt',\n",
    " 'evening_wt',\n",
    " 'midnight_wt',\n",
    " 'watch_all_per_day',\n",
    " 'watch_all_per_week',\n",
    " 'watch_all_per_2weeks',\n",
    " 'watch_all_per_session',\n",
    " 'session_all_per_day',\n",
    " 'pid_SHUA_all_per_day',\n",
    " 'pid_watch_all_per_day',\n",
    " 'duration_per_day',\n",
    " 'longvideo_count_per_day',\n",
    " 'shortvideo_count_per_day',\n",
    " 'coverage_per_week',\n",
    " 'coverage_per_2weeks',\n",
    " 'category_count_unique',\n",
    " 'root_category_count_unique',\n",
    " 'month']\n",
    "\n",
    "df_survey = df_survey[list(common_columns)]\n",
    "df_survey['user_id'] = df_survey['user_id'].astype(str)\n",
    "df_concated['user_id'] = df_concated['user_id'].astype(str)\n",
    "df_survey = df_survey.merge(df_concated[['user_id','addiction_score','soft_criteria','hard_criteria','age','gender','age_segment','age_survey','sex','sex_str','fre_city_level','database','submit_time']],on='user_id',how='left')\n",
    "\n",
    "\n",
    "df_survey['hard_criteria'] = 0\n",
    "\n",
    "# Merge the corresponding values from df_all into past_119_interaction_all\n",
    "df_survey = df_survey.merge(\n",
    "    df_all[['user_id', 'hard_criteria']], \n",
    "    on='user_id', \n",
    "    how='left', \n",
    "    suffixes=('', '_from_df_all')\n",
    ")\n",
    "\n",
    "\n",
    "print(df_survey[~df_survey['hard_criteria'].isna()]['user_id'].nunique())\n",
    "\n",
    "user_ids_to_update = df_survey[(df_survey['watch_all_per_day'] == 0) & \n",
    "                                              (df_survey['month'] == 0)]['user_id'].unique()\n",
    "\n",
    "columns_to_remove = ['hard_criteria','soft_criteria', 'addiction_score', '3_label_criteira']\n",
    "\n",
    "\n",
    "# Filter both datasets to include only the required columns\n",
    "df_survey_filt = df_survey[required_columns]\n",
    "\n",
    "df_10000_filt = df_10000[required_columns]\n",
    "\n",
    "df_all_forlabeling = pd.concat([df_survey_filt, df_10000_filt], ignore_index=True)\n",
    "print(df_all_forlabeling.shape)\n",
    "\n",
    "\n",
    "df_all_forlabeling['pid_0s_per_day_nunique'] = df_all_forlabeling['pid_SHUA_all_per_day'] - df_all_forlabeling['pid_watch_all_per_day']\n",
    "\n",
    "\n",
    "df_all_forlabeling['gender'] = df_all_forlabeling['gender'].map(gender_dict).astype(float)\n",
    "gender_str_dict = {'男': 'M', '女':'F' }\n",
    "df_all_forlabeling['sex_str'] = df_all_forlabeling['sex_str'].map(gender_str_dict)\n",
    "\n",
    "\n",
    "df_all_forlabeling['fre_city_level'] = df_all_forlabeling['fre_city_level'].map(fre_city_level_dict).astype(float)\n",
    "\n",
    "# Define the age_segment bins and corresponding labels\n",
    "bins = [0, 11, 17, 23, 30, 40, 49, np.inf]\n",
    "labels = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Fill missing 'age_segment' using pd.cut(), but only for rows where 'age_segment' is NaN\n",
    "df_all_forlabeling['age_segment'] = np.where(\n",
    "    df_all_forlabeling['age_segment'].isna(), \n",
    "    pd.cut(df_all_forlabeling['age'], bins=bins, labels=labels), \n",
    "    df_all_forlabeling['age_segment']\n",
    ")\n",
    "df_all_forlabeling['age_survey_segment'] = np.nan\n",
    "df_all_forlabeling['age_survey_segment'] = np.where(\n",
    "    df_all_forlabeling['age_survey_segment'].isna(), \n",
    "    pd.cut(df_all_forlabeling['age_survey'], bins=bins, labels=labels), \n",
    "    df_all_forlabeling['age_survey_segment']\n",
    ")\n",
    "\n",
    "df_all_forlabeling = df_all_forlabeling[df_all_forlabeling['month'] <= 9]\n",
    "\n",
    "df_all_forlabeling['month_chronological_order'] = 10 - df_all_forlabeling['month']\n",
    "\n",
    "df_all_forlabeling.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./addict1_xgboost/soft_criteria_xgboost_model_best.pkl', 'rb') as model_file:\n",
    "    best_model = pickle.load(model_file)\n",
    "\n",
    "\n",
    "feature_column = [\n",
    "    \"age\", 'gender', 'watch_all_per_day', 'coverage_per_2weeks',\n",
    "    \"category_count_unique\", \n",
    "    'session_all_per_day', 'midnight_wt',\n",
    "    'morning_wt', \n",
    "    'noon_wt', \n",
    "    'afternoon_wt', \n",
    "    'evening_wt',\n",
    "    # # scrolling+ watch unique videos vs watch >=1 second(s) unique videos\n",
    "    'pid_SHUA_all_per_day', \n",
    "    'pid_watch_all_per_day',\n",
    "]\n",
    "\n",
    "# Check that all required feature columns are present in the dataset\n",
    "missing_features = [feature for feature in feature_column if feature not in df_all_forlabeling.columns]\n",
    "if missing_features:\n",
    "\n",
    "    print(f\"need missing feature columns: {missing_features}\")\n",
    "\n",
    "# Prepare the dataset for prediction\n",
    "d_matrix = xgb.DMatrix(df_all_forlabeling[feature_column])  # Only include required feature columns\n",
    "\n",
    "# Predict the target labels using the loaded model\n",
    "predicted_labels = best_model.predict(d_matrix)\n",
    "\n",
    "# Convert probabilities to binary labels (0 or 1)\n",
    "df_all_forlabeling['preds_soft'] = (predicted_labels >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "with open('./addict1_xgboost/hard_criteria_xgboost_model_severely_mildly_best.pkl', 'rb') as model_file:\n",
    "    best_model = pickle.load(model_file)\n",
    "\n",
    "# Check that all required feature columns are present in the dataset\n",
    "missing_features = [feature for feature in feature_column if feature not in df_all_forlabeling.columns]\n",
    "if missing_features:\n",
    "    print(f\"Missing feature columns detected: {missing_features}\")\n",
    "    # Optionally create the missing feature columns with default values (e.g., 0 or NaN)\n",
    "    for feature in missing_features:\n",
    "        df_all_forlabeling[feature] = 0  # Adjust default value if necessary\n",
    "\n",
    "# Step 1: Filter dataset to include only users where preds_soft == 1\n",
    "soft_criteria_subset = df_all_forlabeling[df_all_forlabeling['preds_soft'] == 1]\n",
    "\n",
    "# Step 2: Prepare the DMatrix for XGBoost prediction using the filtered subset\n",
    "d_matrix = xgb.DMatrix(soft_criteria_subset[feature_column])  # Only include required feature columns\n",
    "\n",
    "# Step 3: Predict the target labels using the loaded model\n",
    "predicted_probs = best_model.predict(d_matrix)\n",
    "\n",
    "# Step 4: Convert probabilities to binary labels (0 or 1) for preds_hard\n",
    "soft_criteria_subset['preds_hard'] = (predicted_probs >= 0.5).astype(int)\n",
    "\n",
    "# Step 5: Merge predictions back into the original dataset\n",
    "# Keep only the 'user_id' and 'preds_hard' columns from the filtered dataset\n",
    "soft_criteria_subset = soft_criteria_subset[['user_id','month', 'preds_hard']]\n",
    "\n",
    "# Merge the predicted 'preds_hard' back into the full dataset\n",
    "df_all_forlabeling = df_all_forlabeling.merge(soft_criteria_subset, on=['user_id','month'], how='left', suffixes=('', '_temp'))\n",
    "\n",
    "# Step 6: Ensure all users have a 'preds_hard' value\n",
    "# Fill NaN values in 'preds_hard' with 0\n",
    "df_all_forlabeling['preds_hard'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 7: Drop the temporary column (if created during merging)\n",
    "if 'preds_hard_temp' in df_all_forlabeling.columns:\n",
    "    df_all_forlabeling.drop(columns=['preds_hard_temp'], inplace=True)\n",
    "\n",
    "# make sure the first month data is updated into it\n",
    "month_0_data = df_all_forlabeling[(df_all_forlabeling['month'] == 0) & (~df_all_forlabeling['soft_criteria'].isna())]\n",
    "df_all_forlabeling.loc[month_0_data.index, 'preds_soft'] = month_0_data['soft_criteria']\n",
    "df_all_forlabeling.loc[month_0_data.index, 'preds_hard'] = month_0_data['hard_criteria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_forlabeling.to_csv('dataset.csv', encoding='utf-8',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-2.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
